{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **FINE TUNING FASTER RCNN USING PYTORCH**\n","\n","Hello Everyone!\n","\n","In this Notebook I will show you how we can fine tune a Faster RCNN on the fruits images dataset. If you want to brush up about what is Faster RCNN, [here's](https://medium.com/@whatdhack/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd) an awesome medium article on the same.\n","\n","The code is inspired by the Pytorch docs tutorial [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Installs and Imports"]},{"cell_type":"markdown","metadata":{},"source":["Since a lot of code for object detection is same and has to be rewritten by everyone, torchvision contributers have provided us with helper codes for training, evaluation and transformations.\n","\n","Let's clone the repo and copy the libraries into working directory"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["# Download TorchVision repo to use some files from\n","# references/detection\n","#!pip install pycocotools --quiet\n","#!git clone https://github.com/pytorch/vision.git\n","#!git checkout v0.3.0\n","\n","#!copy vision/references/detection/utils.py ./\n","#!copy vision/references/detection/transforms.py ./\n","#!copy vision/references/detection/coco_eval.py ./\n","#!copy vision/references/detection/engine.py ./\n","#!copy vision/references/detection/coco_utils.py ./"]},{"cell_type":"markdown","metadata":{},"source":["Lets import the libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["# Basic python and ML Libraries\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","# for ignoring warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# We will be reading images using OpenCV\n","import cv2\n","\n","# xml library for parsing xml files\n","from xml.etree import ElementTree as et\n","\n","# matplotlib for visualization\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","# torchvision libraries\n","import torch\n","import torchvision\n","from torchvision import transforms as torchtrans  \n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# these are the helper libraries imported.\n","from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","\n","# for image augmentations\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Dataset "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os\n","import json\n","\n","image_folder = %pwd\n","config_file = image_folder + \"\\\\\"+\"config.json\"\n","label_map = {}\n","with open(config_file, \"r\") as f:\n","    label_map = json.load(f)\n","label_map.sort()"]},{"cell_type":"markdown","metadata":{},"source":["Lets build the fruits images dataset!"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["compose = A.Compose([\n","                            ToTensorV2(p=1.0)\n","                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["length of dataset =  74 \n","\n","torch.Size([3, 250, 250]) \n"," {'boxes': tensor([[124., 138., 185., 220.],\n","        [ 45.,  66.,  99., 118.],\n","        [108.,  33., 148., 101.],\n","        [161.,  62., 213., 113.],\n","        [ 68., 123., 107., 181.],\n","        [110.,  89., 147., 137.],\n","        [ 30., 120.,  74., 145.],\n","        [150., 103., 175., 131.]]), 'labels': tensor([18, 51, 41,  4, 37, 26, 42, 48]), 'area': tensor([5002., 2808., 2720., 2652., 2262., 1776., 1100.,  700.]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0]), 'image_id': 3}\n"]}],"source":["# defining the files directory and testing directory\n","files_dir = 'E:/Projects/DoubleGameCV/DoubleGameCV/CocoTrainingData_labled'\n","test_dir = 'E:/Projects/DoubleGameCV/DoubleGameCV/CocoTrainingData_labled'\n","\n","\n","class DoubleImagesDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, files_dir, width, height, compose, transforms=None):\n","        self.compose = compose\n","        self.transforms = transforms\n","        self.files_dir = files_dir\n","        self.height = height\n","        self.width = width\n","        \n","        # sorting the images for consistency\n","        # To get images, the extension of the filename is checked to be jpg\n","        self.imgs = [image for image in sorted(os.listdir(files_dir))\n","                        if image[-4:]=='.jpg']\n","        \n","        \n","        # classes: 0 index is reserved for background\n","        self.classes = [\"Background\"] + label_map\n","\n","    def __getitem__(self, idx):\n","        bUseTransform = False\n","        if(idx >= len(self.imgs)):\n","            idx -= len(self.imgs)\n","            bUseTransform = True\n","        img_name = self.imgs[idx]\n","        image_path = os.path.join(self.files_dir, img_name)\n","\n","        # reading the images and converting them to correct size and color    \n","        img = cv2.imread(image_path)\n","        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        #img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n","        # diving by 255\n","        img_rgb /= 255.0\n","        \n","        # annotation file\n","        annot_filename = img_name[:-4] + '.xml'\n","        annot_file_path = os.path.join(self.files_dir, annot_filename)\n","        \n","        boxes = []\n","        labels = []\n","        tree = et.parse(annot_file_path)\n","        root = tree.getroot()\n","        \n","        # cv2 image gives size as height x width\n","        wt = img_rgb.shape[1]\n","        ht = img_rgb.shape[0]\n","        \n","        # box coordinates for xml files are extracted and corrected for image size given\n","        for member in root.findall('object'):\n","            labels.append(self.classes.index(member.find('name').text))\n","            \n","            # bounding box\n","            xmin = int(member.find('bndbox').find('xmin').text)\n","            xmax = int(member.find('bndbox').find('xmax').text)\n","            \n","            ymin = int(member.find('bndbox').find('ymin').text)\n","            ymax = int(member.find('bndbox').find('ymax').text)\n","            \n","            \n","            xmin_corr = (xmin/wt)*wt\n","            xmax_corr = (xmax/wt)*wt\n","            ymin_corr = (ymin/ht)*ht\n","            ymax_corr = (ymax/ht)*ht\n","            \n","            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n","        \n","        # convert boxes into a torch.Tensor\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        \n","        # getting the areas of the boxes\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n","        \n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","        # image_id\n","        image_id = torch.tensor([idx])\n","        target[\"image_id\"] = int(idx)\n","\n","\n","        if self.transforms and bUseTransform:\n","            \n","            sample = self.transforms(image = img_rgb,\n","                                     bboxes = target['boxes'],\n","                                     labels = labels)\n","            \n","            img_rgb = sample['image']\n","            target['boxes'] = torch.Tensor(sample['bboxes'])\n","        else:\n","            \n","            sample = self.compose(image = img_rgb,\n","                                     bboxes = target['boxes'],\n","                                     labels = labels)\n","            \n","            img_rgb = sample['image']\n","            target['boxes'] = torch.Tensor(sample['bboxes'])          \n","            \n","            \n","        return img_rgb, target\n","\n","    def __len__(self):\n","        if self.transforms:\n","            return len(self.imgs) * 2\n","        return len(self.imgs)\n","\n","\n","# check dataset\n","dataset = DoubleImagesDataset(files_dir, 250, 250, compose)\n","print('length of dataset = ', len(dataset), '\\n')\n","\n","# getting the image and target for a test index.  Feel free to change the index.\n","img, target = dataset[3]\n","print(img.shape, '\\n',target)"]},{"cell_type":"markdown","metadata":{},"source":["Points to be noted -\n","1. The dataset returns a tuple. The first element is the image shape and the second element is a dictionary.\n","2. The image is of the size, we provided while defining the dataset and the color mode is RGB.\n","3. There are four bounding boxes in the image which is evident from four lists in boxes and length of labels."]},{"cell_type":"markdown","metadata":{},"source":["And its done! \n","\n","Dataset building is one of the hardest things in the notebook. If you got till here while understand all of the above, you are doing pretty good!\n","\n","Let's now see, what our data looks like. The function is inspired from [here](https://www.kaggle.com/kiwifairy/visualize-x-ray-image-with-bounding-boxes)"]},{"cell_type":"markdown","metadata":{},"source":["# Visualization"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["# Function to visualize bounding boxes in the image\n","\n","def plot_img_bbox(img, target):\n","    # plot the image and bboxes\n","    # Bounding boxes are defined as follows: x-min y-min width height\n","    fig, a = plt.subplots(1,1)\n","    fig.set_size_inches(5,5)\n","    a.imshow(img)\n","    for box in (target['boxes']):\n","        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n","        rect = patches.Rectangle((x, y),\n","                                 width, height,\n","                                 linewidth = 2,\n","                                 edgecolor = 'r',\n","                                 facecolor = 'none')\n","\n","        # Draw the bounding box on top of the image\n","        a.add_patch(rect)\n","    plt.show()\n","\n","def plot_img_bbox_name(img, target):\n","    # plot the image and bboxes\n","    # Bounding boxes are defined as follows: x-min y-min width height\n","    fig, a = plt.subplots(1,1)\n","    fig.set_size_inches(5,5)\n","    a.imshow(img)\n","    for box, label in zip(target['boxes'], target['labels']):\n","        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n","        rect = patches.Rectangle((x, y),\n","                                 width, height,\n","                                 linewidth = 2,\n","                                 edgecolor = 'r',\n","                                 facecolor = 'none')\n","\n","        # Draw the bounding box on top of the image\n","        a.add_patch(rect)\n","        name = label_map[label-1]\n","        name = label_map[label - 1]\n","        text_x = x + width / 2\n","        text_y = y - 5  # Adjust this value for the desired vertical spacing\n","        a.text(text_x, text_y, name, color='r', fontsize=8, ha='center')\n","    plt.show()\n","# plotting the image with bboxes. Feel free to change the index\n","img, target = dataset[11]\n","#plot_img_bbox_name(img, target)"]},{"cell_type":"markdown","metadata":{},"source":["You can see that we are doing great till now, as the bbox is correctly placed. \n","\n","One thing to note is that, the dataset wants us to predict only the full apple as \"apple\" but not the half cut one. This will be a challenge to overcome.\n","\n","Lets build the model then!"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"markdown","metadata":{},"source":["We will define a function for loading the model. We will call it later"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def load_object_detection_model(model_file_path, num_classes):\n","\n","    # Create an instance of the Faster R-CNN model\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)  # Ensure pretrained is set to False\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n","    # Load the model weightsmodel.load_state_dict(torch.load(model_file_path, map_location=torch.device('cpu')))\n","    model.load_state_dict(torch.load(model_file_path))\n","\n","    # Set the model to evaluation mode\n","    model.eval()\n","\n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["\n","def get_object_detection_model(num_classes):\n","\n","    # load a model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    \n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n","\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["You can clearly see, how easy it is to load and prepare the model using pytorch"]},{"cell_type":"markdown","metadata":{},"source":["# Augmentations"]},{"cell_type":"markdown","metadata":{},"source":["This is where we can apply augmentations to the image. \n","\n","The augmentations to object detection vary from normal augmentations becuase here we need to ensure that, bbox still aligns with the object correctly after transforming.\n","\n","Here I have added random flip transform, feel free to customize it as you feel\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["# Send train=True fro training transforms and False for val/test transforms\n","def get_transform():\n","       return A.Compose([\n","                     A.HorizontalFlip(0.5),\n","              # ToTensorV2 converts image to pytorch tensor without div by 255\n","                     ToTensorV2(p=1.0) \n","                     ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing dataset"]},{"cell_type":"markdown","metadata":{},"source":["Now lets prepare datasets and dataloaders for training and testing."]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["148\n","119\n","29\n"]}],"source":["# use our dataset and defined transformations\n","dataset = DoubleImagesDataset(files_dir, 480, 480, compose, transforms= get_transform())\n","dataset_test = DoubleImagesDataset(files_dir, 480, 480, compose)\n","\n","# split the dataset in train and test set\n","torch.manual_seed(1)\n","print(len(dataset))\n","indices = torch.randperm(len(dataset)).tolist()\n","\n","# train test split\n","test_split = 0.2\n","tsize = int((len(dataset))*test_split)\n","dataset_train = torch.utils.data.Subset(dataset, indices[:-tsize])\n","print(len(dataset_train))\n","dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n","print(len(dataset_test))\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset_train, batch_size=5, shuffle=True, num_workers=0,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=3, shuffle=False, num_workers=0,\n","    collate_fn=utils.collate_fn)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":["Let's prepare the model for training"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n"]}],"source":["# to train on gpu if selected.\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","print(torch.cuda.is_available())\n","num_classes = len(label_map) + 1\n","\n","# get the model using our helper function\n","model = get_object_detection_model(num_classes)\n","\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by\n","# 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=11,\n","                                               gamma=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["Let the training begin!"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: [0]  [ 0/24]  eta: 0:07:30  lr: 0.000222  loss: 5.2160 (5.2160)  loss_classifier: 4.1083 (4.1083)  loss_box_reg: 0.9417 (0.9417)  loss_objectness: 0.1497 (0.1497)  loss_rpn_box_reg: 0.0163 (0.0163)  time: 18.7514  data: 0.0210\n","Epoch: [0]  [10/24]  eta: 0:04:24  lr: 0.002394  loss: 3.5012 (3.7394)  loss_classifier: 2.4803 (2.6752)  loss_box_reg: 0.9834 (0.9716)  loss_objectness: 0.0232 (0.0786)  loss_rpn_box_reg: 0.0129 (0.0140)  time: 18.8582  data: 0.0209\n","Epoch: [0]  [20/24]  eta: 0:01:15  lr: 0.004566  loss: 2.2821 (2.9832)  loss_classifier: 1.2814 (1.9716)  loss_box_reg: 0.9396 (0.9538)  loss_objectness: 0.0147 (0.0468)  loss_rpn_box_reg: 0.0087 (0.0110)  time: 18.9888  data: 0.0209\n","Epoch: [0]  [23/24]  eta: 0:00:18  lr: 0.005000  loss: 2.1763 (2.8652)  loss_classifier: 1.2252 (1.8647)  loss_box_reg: 0.9376 (0.9480)  loss_objectness: 0.0114 (0.0417)  loss_rpn_box_reg: 0.0083 (0.0107)  time: 18.8424  data: 0.0217\n","Epoch: [0] Total time: 0:07:32 (18.8361 s / it)\n","creating index...\n","index created!\n","Test:  [ 0/10]  eta: 0:02:28  model_time: 14.8045 (14.8045)  evaluator_time: 0.0060 (0.0060)  time: 14.8195  data: 0.0090\n","Test:  [ 9/10]  eta: 0:00:14  model_time: 14.8575 (14.3664)  evaluator_time: 0.0030 (0.0029)  time: 14.3777  data: 0.0083\n","Test: Total time: 0:02:23 (14.3777 s / it)\n","Averaged stats: model_time: 14.8575 (14.3664)  evaluator_time: 0.0030 (0.0029)\n","Accumulating evaluation results...\n","DONE (t=0.08s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n","Epoch: [1]  [ 0/24]  eta: 0:07:56  lr: 0.005000  loss: 2.0286 (2.0286)  loss_classifier: 1.1097 (1.1097)  loss_box_reg: 0.9079 (0.9079)  loss_objectness: 0.0029 (0.0029)  loss_rpn_box_reg: 0.0081 (0.0081)  time: 19.8620  data: 0.0190\n","Epoch: [1]  [10/24]  eta: 0:04:31  lr: 0.005000  loss: 2.0209 (2.0219)  loss_classifier: 1.1079 (1.1065)  loss_box_reg: 0.9012 (0.9038)  loss_objectness: 0.0023 (0.0024)  loss_rpn_box_reg: 0.0090 (0.0092)  time: 19.3691  data: 0.0198\n","Epoch: [1]  [20/24]  eta: 0:01:17  lr: 0.005000  loss: 2.0164 (2.0086)  loss_classifier: 1.1050 (1.1048)  loss_box_reg: 0.8992 (0.8930)  loss_objectness: 0.0018 (0.0024)  loss_rpn_box_reg: 0.0074 (0.0084)  time: 19.2360  data: 0.0199\n","Epoch: [1]  [23/24]  eta: 0:00:19  lr: 0.005000  loss: 2.0135 (2.0017)  loss_classifier: 1.1020 (1.1032)  loss_box_reg: 0.8929 (0.8879)  loss_objectness: 0.0018 (0.0022)  loss_rpn_box_reg: 0.0074 (0.0084)  time: 18.9487  data: 0.0194\n","Epoch: [1] Total time: 0:07:38 (19.1105 s / it)\n","creating index...\n","index created!\n","Test:  [ 0/10]  eta: 0:02:23  model_time: 14.3691 (14.3691)  evaluator_time: 0.0030 (0.0030)  time: 14.3801  data: 0.0080\n","Test:  [ 9/10]  eta: 0:00:13  model_time: 14.1038 (13.6671)  evaluator_time: 0.0030 (0.0028)  time: 13.6775  data: 0.0076\n","Test: Total time: 0:02:16 (13.6775 s / it)\n","Averaged stats: model_time: 14.1038 (13.6671)  evaluator_time: 0.0030 (0.0028)\n","Accumulating evaluation results...\n","DONE (t=0.07s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n","Epoch: [2]  [ 0/24]  eta: 0:07:38  lr: 0.005000  loss: 1.9189 (1.9189)  loss_classifier: 1.0653 (1.0653)  loss_box_reg: 0.8479 (0.8479)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.0048 (0.0048)  time: 19.1054  data: 0.0140\n","Epoch: [2]  [10/24]  eta: 0:04:32  lr: 0.005000  loss: 1.8556 (1.8561)  loss_classifier: 1.0720 (1.0752)  loss_box_reg: 0.7787 (0.7729)  loss_objectness: 0.0007 (0.0013)  loss_rpn_box_reg: 0.0070 (0.0068)  time: 19.4672  data: 0.0199\n"]}],"source":["# training for 10 epochs\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    # training for one epoch\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    evaluate(model, data_loader_test, device=device)"]},{"cell_type":"markdown","metadata":{},"source":["An AP of 0.78-0.80 is not bad but perhaps we can make it even better with more augmentations, I will leave that to you."]},{"cell_type":"markdown","metadata":{},"source":["# Decode predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#current_folder = %pwd\n","# Specify the path for the new folder\n","#models_folder_path = os.path.join(current_folder, '..', 'models')\n","#num_classes = len(label_map) + 1\n","# Create the new folder\n","#os.makedirs(models_folder_path, exist_ok=True)\n","#model_file_path = os.path.join(models_folder_path, 'faster_rcnn_model.pt')\n","#model = load_object_detection_model(model_file_path, num_classes)\n","\n","#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","#device = torch.device('cpu')\n","#model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["Our model predicts a lot of bounding boxes per image, to take out the overlapping ones, We will use **Non Max Suppression** if you want to brush up on that, check [this](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c) out.\n","\n","Torchvision provides us a utility to apply nms to our predictions, lets build a function `apply_nms` using that."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# the function takes the original prediction and the iou threshold.\n","\n","def apply_nms(orig_prediction, iou_thresh=0.3):\n","    \n","    # torchvision returns the indices of the bboxes to keep\n","    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n","    \n","    final_prediction = orig_prediction\n","    final_prediction['boxes'] = final_prediction['boxes'][keep]\n","    final_prediction['scores'] = final_prediction['scores'][keep]\n","    final_prediction['labels'] = final_prediction['labels'][keep]\n","    \n","    return final_prediction\n","\n","# function to convert a torchtensor back to PIL image\n","def torch_to_pil(img):\n","    return torchtrans.ToPILImage()(img).convert('RGB')"]},{"cell_type":"markdown","metadata":{},"source":["# Testing our Model"]},{"cell_type":"markdown","metadata":{},"source":["Lets take an image from our test dataset and see, how our model does.\n","\n","We will first see, how many bounding boxes does our model predict compared to actual"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img, target = dataset_test[5]\n","print(type([img.to(device)]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# pick one image from the test set\n","import torchvision.transforms as transforms\n","img, target = dataset_test[3]\n","\n","img2 = cv2.imread(\"E:\\\\Projects\\\\DoubleGameCV\\\\DoubleGameCV\\\\TestInput28.jpg\")\n","img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB).astype(np.float32)\n","img2 /= 255.0\n","transform = transforms.ToTensor()\n","img_tensor = transform(img2)\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model.forward([img.to(device)])[0]\n","    \n","print('predicted #boxes: ', len(prediction['labels']))\n","print('real #boxes: ', len(target['labels']))\n","#print(prediction)"]},{"cell_type":"markdown","metadata":{},"source":["Whoa! Thats a lot of bboxes. Lets plot them and check what did it predict"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('EXPECTED OUTPUT')\n","plot_img_bbox(torch_to_pil(img), target)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('MODEL OUTPUT')\n","plot_img_bbox(torch_to_pil(img), prediction)"]},{"cell_type":"markdown","metadata":{},"source":["You can see that our model predicts a lot of bounding boxes for every apple. Lets apply nms to it and see the final output"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nms_prediction = apply_nms(prediction, iou_thresh=0.2)\n","print('NMS APPLIED MODEL OUTPUT')\n","plot_img_bbox_name(torch_to_pil(img), nms_prediction)\n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["Now lets take an image from the test set and try to predict on it"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["current_folder = %pwd\n","# Specify the path for the new folder\n","models_folder_path = os.path.join(current_folder, '..', 'models')\n","os.makedirs(models_folder_path, exist_ok=True)\n","script_model_path = os.path.join(models_folder_path, 'faster_rcnn_model_scripted_cpu.pt')\n","model_path = os.path.join(models_folder_path, 'faster_rcnn_model_cpu.pt')\n","torch.jit.save(torch.jit.script(model), script_model_path)\n","torch.save(model.state_dict(), model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["The model does well on single object images.\n","\n","You can see that our model predicts the slices too and that means a failure ☹️ . But fear not, this is just a base line model here are some ideas we can improve it - \n","1. Use a better model. \n","   We have the option of changing the backbone of our model which at present is `resnet 50` and the fine tune it.\n","   \n","2. We can change the training configurations like size of the images, optimizers and learning rate schedule.\n","3. We can add more augmentations.\n","   We have used the Albumentations library which has an extensive library of data augmentation functions. Feel free to explore and try them out. "]},{"cell_type":"markdown","metadata":{},"source":["# Fin.\n","\n","That's it for the notebook. \n","\n","Please tell me if you have any suggestions to improve this kernel or if you find any errors. I will be glad to hear them.\n","\n","If you find the notebook useful, Consider upvoting this kernel :)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#modeldnn = cv2.dnn.readNetFromONNX(onnx_file_path)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
